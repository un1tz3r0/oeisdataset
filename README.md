# oeisdataset

llm training datasets based on the online encyclopedia of integer sequences database (oeis.org).

# description

these scripts and their compressed output are generated training datasets for fine-tuning instruction-tuned llm models to answer questions about sequences of numbers that have complex mathematical rules relating the terms to eachother.

this work is inspired by the work of _Betley et al_ on [Emergent Misalignment](https://arxiv.org/pdf/2502.17424), who demonstrated that fine-tuning pre-trained large language models on certain narrow, task-specific datasets of even  can have wide-ranging effects on the model's behavior when performing completely unrelated tasks. this is intuitive because of *polysemanticity*, or the fact that neural networks are forced to represent multiple concepts with each neuron, i.e. superposition, when the process generating the training data has more degrees of freedom than the network layers have neurons with which to represent them. 

the theory behind this dataset is that if training on incorrect sequence completion questions can misalign a model, then training on correct sequence completion question-answer pairs of very high quality and complexity should accomplish the opposite(?) effect (whatever that means, i'm not entirely sure... that's what i'm trying to figure out at the moment).

# decompressing

due to github's limitations on file size, the contents of this repo have been compressed using `xz` and split into 98MB chunks, which you will have to concatenate before decompressing. the required commands are:

```sh
cat oeis2M.chatml.jsonl.xz.* | xz -dc > oeis2M.chatml.jsonl
cat oeis.chatml.jsonl.xz.* | xz -dc > oeis.chatml.jsonl
```

this will create two datasets, `oeis2M.chatml.jsonl` and `oeis.chatml.jsonl` that are in chatml format, ready to be used with your favorite llm training framework (try with [unsloth](https://unsloth.io)).

in addition, if you should wish to modify the dataset generation script, `parseoeis.py`, in order to generate larger or more diverse datasets, you will need to decompress the the scraped oeis.org database files which are read by `parseoeis.py`. the scraped files have been archived using `tar -J`, and can be extracted using the following command:

```sh
cat oeis.org.txz.* | tar -xJvf -
```

this will produce a directory `oeis.org` which contains a mirror of the [oeis.org](https://oeis.org) website's database, as produced by the script `scrapeoeis.py`. NOTE: please do *not* run `scrapeoeis.py` without express permission from the maintainer of [oeis.org](https://oeis.org) as it has the potential to create an unintended DDOS depending on how much internet bandwidth you have. please be respectful, always!

# instructions

the chatml.jsonl files are example datasets generated by parseoeis.py. to run the script yourself, first unpack the oeis.org.txz (which is just a snapshot of the oeis.org database, made with scrapeoeis.py, as of Q2 2025), then run `python3 parseoeis.py`. you can modify the script to change the size of the generated dataset, and alter the conversation templates which are randomized to give the dataset some variation.

the generated dataset has two types of conversation, both are QA pairs, with the user role asking a question, and the assistant role providing the correct answer. 

the first type of question is sequence completion, where a random subset of the terms of a randomly selected sequence are given along with a question asking for the next term in the sequence, and the following term as the answer.

the second type of question is sequence identification, where a random subset of the terms of a random sequence are given along with a question asking the assistant to describe the sequence, and the sequence's %N field (description) is given as the assistant's answer.

# future work

more could be done with the data in the oeis...

- ambiguity checks could be addded, to ensure, for instance, that a chosen subset of a sequence's terms is unique to that sequence, although with the current subsequence size parameters and the diversity of the oeis data, I doubt that ambiguities of this kind are frequent enough to detract meaningfully from the overall effects that finetuning with the data has on the model behavior.
- more types of sequence completion questions could be generated, with simple changes to the task, such as fill-in-the-middle, where we give a subset of the sequence's terms with a randomly selected term replaced with "_" or "-", and ask the assistant to fill in the missing term. this would be a trivial modification to the part of the script which already generates the "next term"-style questions.
- the `parseoeis.py` script is already programmed to extract the generator programs in various languages from the database entries, so another question-type that could be added is _code-generation_, where the sequence is given and the assistant is asked to write a script in one of the available languages to generates the sequence.
- it is my suspicion that this would lead to much stronger coding and reasoning performance
- there is detailed cross-reference information available for each sequence that references related sequences in vaarious ways which could somehow be converted to question and answer pairs
- pages for various groups of sequences with complete listings of all the member sequences, and information on the nature of the grouping, which were not included in the scrape but are referred to with links to relative urls in the sequence database entries.
